###############################################################################
# Author: Jiyang Yu, Xinge Wang
# 2019.8.2
# Main functions
###############################################################################
#' @import Biobase arm MCMCglmm kableExtra RColorBrewer readxl plyr dplyr stringr readr data.table
library(Biobase)
library(plyr)
library(dplyr)
library(stringr)
library(readxl)
library(arm)
library(MCMCglmm)
library(kableExtra)
library(RColorBrewer)
library(readr)
library(data.table)
library(NetBID2)


#' Manipulation of Working Directories for ScreenBEAM2 analysis
#'
#' \code{ScreenBEAM.dir.create} is used to help users create an organized working directory for the Functional Genomics Screening Projects.
#' It creates a hierarchcial working directory and returns a list contains this directory information.
#'
#' @param projecet_main_dir character, name or absolute path of the main working directory.
#' @param lib_name character, name of the sh/sgRNA library. Each main working directory can contain multiple library sub-project folder.
#' @param DATE logical, if TRUE, current date information will be added to the sub-project folder name. Default is TRUE.
#'
#' @details This function needs users to define the main working directory and the libraryâ€™s name. It creates
#' a main working directory with a subdirectory of the project(each library is a project). It also automatically creates 4
#' subfolders (fastq, metadata, library and output) within the project folder. fastq/, storing unzipped fastq files of each sample;
#' metadata/, storing the experiment design, describe detailed information of each sample; library/, storing sh/sgRNA library csv file and its fasta file (for detail, please check demo).
#' output/ folder contains 3 subfolder to store intermediate results generated by ScreenBEAM2 analysis. They are mapping/, QC/ and DR/.
#'
#' Inside mapping/ there are 3 subfolders as well.
#' mapping/Step1_unique_fast, mapping/Step2_blat and mapping/Step3_raw_count. mapping/Step1_unique_fast stores unique fasta read from each sample's fastq file. mapping/Step2_blat stores
#' blat result when using unique fasta file and library fasta file. mapping/Step3_raw_count stores 3 raw count table created later in the pipeline.
#' This function also returns a list object (analysis.par in the demo) with directory information wrapped inside.
#' This list is an essential for ScreenBEAM2 analysis, all the important intermediate data generated later will be wrapped inside.
#'
#' QC/ is used to store quality control analysis of the mapping result. Including library check and mapping statistics.
#'
#' DR/ is used to store the Bayesian hierarchical meta analysis result.
#'
#' @return Return a list object, containing all the paths of the created folder.
#' @export
ScreenBEAM.dir.create <- function(project_main_dir=NULL, lib_name=NULL, DATE=TRUE){
  if(exists('analysis.par')==TRUE){message('analysis.par is occupied in the current session,please manually run: rm(analysis.par) and re-try, otherwise will not change !');
    return(analysis.par)}
  if(is.null(project_main_dir)==TRUE){message('project_main_dir required, please input and re-try!');return(FALSE)}
  if(is.null(lib_name)==TRUE){message('lib_name required, please input and re-try!');return(FALSE)}
  analysis.par <- list()
  analysis.par$main.dir <- project_main_dir
  analysis.par$lib.name <- lib_name
  date<-format(Sys.Date(), "%Y%m%d")
  if(!DATE){analysis.par$out.dir <- sprintf('%s/%s/',paste0("Project_",analysis.par$main.dir,analysis.par$lib.name))
  }else{analysis.par$out.dir <- sprintf('%s/%s/',analysis.par$main.dir,paste0("Project_",analysis.par$lib.name,"_",date))}
  if (!dir.exists(analysis.par$out.dir)) {
    dir.create(analysis.par$out.dir, recursive = TRUE)
  }
  analysis.par$out.dir.fastq <- paste0(analysis.par$out.dir, '/fastq/')
  if (!dir.exists(analysis.par$out.dir.fastq)) {
    dir.create(analysis.par$out.dir.fastq, recursive = TRUE) ## directory for fastq reads
  }
  analysis.par$out.dir.library <- paste0(analysis.par$out.dir, '/library/')
  if (!dir.exists(analysis.par$out.dir.library)) {
    dir.create(analysis.par$out.dir.library, recursive = TRUE) ## directory for shRNA or sgRNA library files
  }
  analysis.par$out.dir.metadata <- paste0(analysis.par$out.dir, '/metadata/')
  if (!dir.exists(analysis.par$out.dir.metadata)) {
    dir.create(analysis.par$out.dir.metadata, recursive = TRUE) ## directory for meta data related files
  }
  analysis.par$out.dir.output <- paste0(analysis.par$out.dir, '/output/')
  if (!dir.exists(analysis.par$out.dir.output)) {
    dir.create(analysis.par$out.dir.output, recursive = TRUE) ## directory for all outputfiles
  }
  analysis.par$out.dir.output.mapping <- paste0(analysis.par$out.dir.output, '/mapping/')
  if (!dir.exists(analysis.par$out.dir.output.mapping)) {
    dir.create(analysis.par$out.dir.output.mapping, recursive = TRUE) ## directory for all outputfiles
  }
  analysis.par$out.dir.output.mapping.step1 <- paste0(analysis.par$out.dir.output.mapping, '/Step1_unique_fasta/')
  if (!dir.exists(analysis.par$out.dir.output.mapping.step1)) {
    dir.create(analysis.par$out.dir.output.mapping.step1, recursive = TRUE) ## directory for all outputfiles
  }
  analysis.par$out.dir.output.mapping.step2 <- paste0(analysis.par$out.dir.output.mapping, '/Step2_blat/')
  if (!dir.exists(analysis.par$out.dir.output.mapping.step2)) {
    dir.create(analysis.par$out.dir.output.mapping.step2, recursive = TRUE) ## directory for all outputfiles
  }
  analysis.par$out.dir.output.mapping.step3 <- paste0(analysis.par$out.dir.output.mapping, '/Step3_raw_count/')
  if (!dir.exists(analysis.par$out.dir.output.mapping.step3)) {
    dir.create(analysis.par$out.dir.output.mapping.step3, recursive = TRUE) ## directory for all outputfiles
  }
  analysis.par$out.dir.output.QC <- paste0(analysis.par$out.dir.output, '/QC/')
  if (!dir.exists(analysis.par$out.dir.output.QC)) {
    dir.create(analysis.par$out.dir.output.QC, recursive = TRUE) ## directory for all outputfiles
  }

  analysis.par$out.dir.output.DR <- paste0(analysis.par$out.dir.output, '/DR/')
  if (!dir.exists(analysis.par$out.dir.output.DR)) {
    dir.create(analysis.par$out.dir.output.DR, recursive = TRUE) ## directory for all outputfiles
  }
  message(sprintf('Analysis space created, please check %s',analysis.par$out.dir))
  return(analysis.par)
}


#' Check sh/sgRNA library and Remove duplicated/substring sh/sgRNA
#'
#' \code{ScreenBEAM.check.lib} is a pre-processing function for \code{ScreenBEAM.raw.count}.
#' It requires the file path of the library file (.csv file, with 1st column to be RNAid, 2nd column RNA sequence, 3rd column target gene name).
#' It will remove the duplicated/substring sh/sgRNA and store it into a list.
#'
#' @param lib.path character, the file path of the library. Should be a csv file. The 1st column is RNAid, 2nd column RNA sequence and 3rd column target gene name.
#'
#' @return Return a list object, containing whole library information after removed duplicates; The most command sh/sgRNA length; Total sh/sgRNA number; All the sh/sgRNA names;
#' Each sh/sgRNA length; Removed sh/sgRNA information.
#'
#' @export
ScreenBEAM.check.lib<-function(lib.path){
  # Read in library file
  lib<-read.csv(lib.path, colClasses = "character")

  # Check if columns are correct
  check.columns.lib(lib)
  lib[,2]<-as.character(lib[,2])
  lib$nchar<-nchar(lib[,2])
  lib.stat<-sort(table(lib$nchar), decreasing = T)
  # Some library statistics need to return
  lib.stat.list<-list() #master list
  substring.df<-NULL # dataframe will store the duplicated/substring sh/sgRNAs information
  substring.id<-c() # index of the duplicated/substring sh/sgRNAs information

  # Remove duplicated sgRNA and substring sgRNA, and print Report
  if(length(names(lib.stat))==1){
    # If all the sh/sgRNAs have the same length
    dup.id<-which(duplicated(lib[,2]))
    print(paste0("Number of duplicated sgRNA is: ", length(dup.id)))
    if(length(dup.id)!=0){
      print("Remove duplicated sgRNAs. Please manually check to make sure they are targeting the same gene.")
      substring.id<-c(substring.id, dup.id)
      dup.df<-lib[duplicated(lib[,2]),]
      dup.df$case<-"Duplicated"
      l<-list(substring.df, dup.df)
      substring.df<-rbindlist(l)
      lib<-lib[!duplicated(lib[,2]),]
    }
  } else{
    # If not all sh/sgRNAs have the same length
    # FIRST find the duplicated ones
    dup.id<-which(duplicated(lib[,2]))
    print(paste0("Number of duplicated sgRNA is: ", length(dup.id)))
    if(length(dup.id)!=0){
      print("Remove duplicated sgRNAs. Please manually check to make sure they are targeting the same gene.")
      substring.id<-c(substring.id, dup.id)
      dup.df<-lib[duplicated(lib[,2]),]
      dup.df$case<-"Duplicated"
      l<-list(substring.df, dup.df)
      substring.df<-rbindlist(l)
      lib<-lib[!duplicated(lib[,2]),]
    }

    # SECOND find not most common seq and check if substring sh/sgRNA are there
    # Recalculate lib.stat
    lib.stat<-sort(table(lib$nchar), decreasing = T)
    # Seperate the most common length
    mode.len<-as.numeric(names(lib.stat)[1])
    mode.id<-which(lib$nchar==mode.len)
    # Seperate the not so common length
    pattern.len<-as.numeric(names(lib.stat)[2:length(names(lib.stat))])
    pattern<-c()
    pattern.id<-c()
    for(len in pattern.len){
      pattern.id<-c(pattern.id,which(lib$nchar==len))
      pattern<-c(pattern, lib[pattern.id,2])
    }
    # If there is a substring detected, we keep the longer one
    for(i in 1:length(pattern)){
      # check if they are substring to the longer RNA
      if(nchar(pattern[i])<=mode.len){
        # if pattern is shorter, then keep the longer one
        match.id<-which(!is.na(str_match(lib[,2], pattern[i]))) #remove.id is the
        if(length(match.id)<=1) next
        match.id<-match.id[!match.id %in% pattern.id[i]]
        for(id in match.id){
          print(paste(paste(lib[pattern.id[i],1],lib[pattern.id[i],2],lib[pattern.id[i],3]), " is a substring of ", paste(lib[id,1],lib[id,2],lib[id,3])))
          substring.id<-c(substring.id, pattern.id[i])
          substring.df<-rbind(substring.df,c(unname(unlist(lib[pattern.id[i],])),"Substring"))
        }
      } else{
        match.id<-which(!is.na(str_match(pattern[i], lib[,2])))
        match.id<-match.id[!match.id %in% pattern.id[i]]
        for(id in match.id){
          p.id<-as.integer(pattern.id[i])
          print(paste(paste(lib[id,1],lib[id,2],lib[id,3]), " is a substring of ", paste(lib[pattern.id[i],1],lib[pattern.id[i],2],lib[pattern.id[i],3])))
          substring.id<-c(substring.id, pattern.id[i])
          substring.df<-rbind(substring.df, c(unname(unlist(lib[id,])),"Substring"))
        }
      }
    }
  }

  # If there are duplicated/substring sh/sgRNA existed in the library, we remove them and keep record
  if(length(substring.id)>0){
    print("Removing:")
    print(paste0(head(lib[substring.id,1])))
    print(paste0("...", length(substring.id), " duplicated/substring sh/sgRNAs."))
    lib<-lib[-c(substring.id),]
  }
  # Wrap stuff to lib.stat.list()
  lib.stat.list$lib<-lib
  lib.stat.list$RNA.len<-as.numeric(names(sort(table(lib$nchar), decreasing = T))[1])
  lib.stat.list$RNA.number<-nrow(lib)
  lib.stat.list$RNA.list<-lib[,1]
  lib.stat.list$RNA.len.list<-lib$nchar
  lib.stat.list$removed.RNA<-substring.df

  return(lib.stat.list)
}


#' Create raw count table for NGS data
#'
#' \code{ScreenBEAM.raw.count} takes a list object, which contains all the paths of library information, unique fasta read path information and blat result path information.
#' This function calculates basic statistics for the FASTQ raw reads, checks library quality, calculates mapping statistics and also collecting raw count numbe from blat
#' result tables. This function can be run directly, or run step by step from its source code.
#'
#' @param analysis.par list, the master list storing all the information along ScreenBEAM2 analysis.
#'
#' @return Return an updated list object, wrapping the raw count table information inside.
#'
#' @export
ScreenBEAM.raw.count<-function(analysis.par){
  ##### This function can be run directly or step by step

  # Check if there is fastq files ready
  fastq.path.list<-list.files(analysis.par$out.dir.fastq, '*.fastq', recursive=T, full.names = T)
  if(length(fastq.path.list)==0) stop("Please put unzipped fastq files in the right path. Type analysis.par$out.dir.fastq to check the path.")
  # Check if library fasta file is ready
  lib.csv.path<-list.files(analysis.par$out.dir.library, '.csv', recursive = T, full.names = T)
  if(length(lib.csv.path)>1){
    # If multiple csv file are in the folder, pick the one with the shortest name
    print(paste0("Multiple csv library files in the folder ", analysis.par$out.dir.library))
    shorest.name.info<-which.min(sapply(lib.csv.path, nchar))
    shorest.name.id<-as.numeric(shorest.name.info)
    lib.csv.path<-lib.csv.path[shorest.name.id]
    print(paste0("Automatic picking file ", lib.csv.path))
  }
  if(length(lib.csv.path)==0) stop("Please put library csv file in the right path. Type analysis.par$out.dir.library to check the path.")
  lib.fasta.path<-list.files(analysis.par$out.dir.library, '.fa', recursive = T, full.names = T)
  if(length(lib.fasta.path)==0) stop("Please create fasta file for your library.")

  ####################### STEP 0: Get statistics about FASTQ files and samples #######################
  # Get basic statistics for FASTQ reads
  print("Step 0: get some basic statistic about the samples FASTQ reads.")
  reads.stat<-get_reads_info(reads.path = fastq.path.list, file.extent = ".fastq", every.n.line = 4)
  # In case one need to re-run, previous information in the analysis.par will be over-write
  if(sum(names(analysis.par)%in%names(reads.stat))!=0){analysis.par<-analysis.par[-which(names(analysis.par)%in%names(reads.stat))]}
  analysis.par<-append(analysis.par, reads.stat)

  # Get basic statistics of library and remove repeated or substring sgRNAs
  lib.stat.list<-ScreenBEAM.check.lib(lib.csv.path)
  if(!is.null(lib.stat.list$removed.RNA)){
    # If there are duplicated sh/sgRNA in the library, we will create a new library csv file
    print("Update the library file and save the new library as csv file.")
    write.csv(lib.stat.list$lib, file = paste0(analysis.par$out.dir.library,lib.name,"_new.csv", row.names=FALSE, quote=FALSE))
  }
  if(sum(names(analysis.par)%in%names(lib.stat.list))!=0){analysis.par<-analysis.par[-which(names(analysis.par)%in%names(lib.stat.list))]}
  analysis.par<-append(analysis.par, lib.stat.list)

  ####################### STEP 1: Create unique fasta #######################
  print("Step 1: create unique FASTA file from FASTQ files.")
  create_unique_readfile(fastq.path.list, out.path = analysis.par$out.dir.output.mapping.step1)
  fasta.path.list<-list.files(analysis.par$out.dir.output.mapping.step1, '*.fa', recursive=T, full.names = T)
  sample.unique.reads.count<-c()
  for(path in fasta.path.list){
    sample.unique.reads.count<-c(sample.unique.reads.count, get_read_count(path, every.n.line = 2, file.extent = ".fa"))
  }
  # UPDATE analysis.par list
  analysis.par$fasta.path.list<-fasta.path.list
  analysis.par$sample.unique.reads.count<-sample.unique.reads.count

  ####################### STEP 2: Run blat mapping using unique reads fasta files and library fastat file #######################
  blat.stat<- run.blat(analysis.par, query = lib.fasta.path)
  # UPDATE analysis.par
  if(sum(names(analysis.par)%in%names(blat.stat))!=0){analysis.par<-analysis.par[-which(names(analysis.par)%in%names(blat.stat))]}
  analysis.par<-append(analysis.par, blat.stat)

  ####################### STEP 3: Collect count table #######################
  raw.count.list<-get_raw_count(analysis.par, save.data.every.run = T) # save.data.every.run=T will save raw.count.list when complete every sample in /output/mapping/Step3
  collect.coutdf.list<-write_count_table(raw.count.list, analysis.par) # combine all the tables into one and write 3 files, count.csv; summary.csv; count.dist.csv
  # UPDATE analysis.par
  if(sum(names(analysis.par)%in%names(collect.coutdf.list))!=0){analysis.par<-analysis.par[-which(names(analysis.par)%in%names(collect.coutdf.list))]}
  analysis.par<-append(analysis.par,collect.coutdf.list)
  return(analysis.par)
}


#' RNA level meta-analysis of high-throughput Functional Genomics Screening analysis
#'
#' \code{ScreenBEAM.rna.level} takes a ".tsv" table file with samples raw count number, user needs to assign the control sample's name and case sample's name vector.
#' It will do a rna-level meta-analysis of screening data.
#'
#' @param input.file, character, the file path of ".tsv" table with raw count number for each sample (column) and sh/sgRNA (row).
#' @param control.samples, vector of character, the control samples names, this information should match the column names of input.file. For example, c(T0_A, T0_B).
#' @param case.samples, vector of character, the case samples names, this information should match the column names of input.file. For example, c(T16_A, T16_B).
#' @param control.groupname, character, the name of your control samples' group name. For example, T0.
#' @param case.groupname, character, the name of your case samples' group name. For example, T16.
#' @param gene.columnId, integer, the number of the column which store the gene name. By default, 2.
#' @param filterLowCount, logical, if TRUE, will remove rnas with low count, based on \code{filterBy} and \code{count.cutoff}. Default is TRUE.
#' @param filterBy, logical, if TRUE, will remove rnas with low count based on "control" samples or "case" samples. Default is "control".
#' @param count.cutoff, integer, the threshold of removing low count. Default is 16.
#' @param do.normalization, logical, if TRUE, a scaled normalization will be performed for each sample. To quantify this scale, user need to assign a value to \code{total}.
#' Default is TRUE.
#' @param total, integer, need to be larger than the colSums of the raw count table. Default is 1e6.
#' @param do.log2, logical, if TRUE, the expression data will be log2 transformed.
#' @param pooling, character, pooling method. Either "full" or "partial". Default is "full".
#' @param family, function, default if gaussian.
#' @param estimation.method, character, estimation model. Either "Bayesian" or "MLE". Default is "Bayesian".
#'
#' @return Return a data frame containing all the rna-level meta-analysis statistical values. Including sh/sgRNA ID, log2FC, p.value and so on.
#'
#' @export
ScreenBEAM.rna.level<-function(input.file, control.samples, case.samples, control.groupname, case.groupname, gene.columnId=2,
                               filterLowCount=TRUE, filterBy = 'control', count.cutoff = 16, do.normalization=TRUE, total=1e6,
                               do.log2=TRUE, pooling ="full", family=gaussian, estimation.method='Bayesian'){
  require(NetBID2)
  # Wrap the count table into eset, we don't perform any NORMALIZATION or LOG2 transform here
  eset<-generateEset.ScreenBEAM(input.file = input.file, control.samples = control.samples,
                                case.samples = case.samples, control.groupname = control.groupname,
                                case.groupname = case.groupname, gene.columnId = gene.columnId)


  # Check case and sample group name existence
  if(!case.groupname%in%pData(eset)$group)
    stop(paste('No group.case \'',group.case,'\' is defined in pData(eset)$group!\n',sep=''))
  if(!control.groupname%in%pData(eset)$group)
    stop(paste('No group.ctrl \'',group.ctrl,'\' is defined in pData(eset)$group!\n',sep=''))

  # filterout low count by count.cutoff
  if(filterLowCount){
    sel<-grep(filterBy, pData(eset)$condition)
    if(length(sel)>1){
      eset.sel<-eset[apply(exprs(eset[,sel]),1,median)>=count.cutoff,]
    }else{
      # For the case only 1 sample without replicates
      eset.sel<- eset[exprs(eset[,sel])>=count.cutoff,]
    }
  }

  # perform scaled normalization to each sample
  if(do.normalization){
    # MAKE SURE ALL POSITIVE
    pseudoCount<-ifelse(all(exprs(eset.sel)>0),0,1)
    if(pseudoCount!=0){print("There are 0 expression value, pesudoCount=1 will be added.")}

    if(all(apply(exprs(eset.sel),2,sum)<total)){
      total<-total
    }else{
      print("WARNING!!!! Not all colSum are larger than total, please give a larger total number!!!")
      print("Scaled normalization to each sample FAILED! Returning unscaled data.")
      total<-NULL
    }
    exprs(eset.sel)<-as.matrix(normalize.scale(exprs(eset.sel), total = total, pseudoCount = pseudoCount))
  }

  # perform log2 transfrom to all expression value
  if(do.log2){
    pseudoCount<-ifelse(all(exprs(eset.sel)>0),0,1)
    if(pseudoCount!=0){print("There are 0 expression value, pesudoCount=1 will be added.")}
    exprs(eset.sel)<-log2(exprs(eset.sel)+pseudoCount)
    logTransformed <- FALSE
  }

  # Use NetBID2 to compare
  if(dim(exprs(eset.sel))[2]<=2){
    print("Only 2 samples are comparing, will only calculate the log2FC without bid function.")
    d<-exprs(eset.sel)
    d<-data.frame(t(d))
    comp<-as.factor(c(0,1))
    dat<-data.frame(
      response=
        c(unlist(d[comp==levels(comp)[1],]),unlist(d[comp==levels(comp)[2],]))
      ,
      treatment=
        factor(c(rep(levels(comp)[1],sum(comp==levels(comp)[1])*ncol(d)),rep(levels(comp)[2],sum(comp==levels(comp)[2])*ncol(d))))
      ,
      probe=
        factor(c(rep(colnames(d),each=sum(comp==levels(comp)[1])),rep(colnames(d),each=sum(comp==levels(comp)[2]))))
    )
    pseudoCount<-ifelse(all(exprs(eset.sel)>0),0,1)
    FC.df<-ddply(dat, .(probe), function(df){
      FC.val<-FC(df$response,df$treatment,logTransformed=do.log2,log.base=2,average.method='geometric',pseudoCount=pseudoCount)
    })
    names(FC.df)<-c("probe","log2FC")
    avg.names<-paste0("Ave.",colnames(exprs(eset.sel)))
    exprs<-data.frame(exprs(eset.sel)[,1], exprs(eset.sel)[,2])
    colnames(exprs)<-avg.names
    de<-data.frame(ID=FC.df$probe,log2FC=FC.df$log2FC, exprs)
  } else{
    print("More than 2 samples are comparing, call bid function.")
    phe_info<-pData(eset.sel)
    G1<-rownames(phe_info)[which(phe_info$`group`==case.groupname)]
    G0<-rownames(phe_info)[which(phe_info$`group`==control.groupname)]
    comp <- c(rep(1, length.out = base::length(G1)), rep(0, length.out = base::length(G0)))
    de <- NetBID2::getDE.BID.2G(eset=eset.sel,G1=G1,G0=G0,G1_name=case.groupname,G0_name=control.groupname, pooling = pooling,
                                logTransformed = FALSE,family = family, method = estimation.method)
  }
  return(de)
}

#' GENE level meta-analysis of high-throughput Functional Genomics Screening analysis
#'
#' \code{ScreenBEAM.gene.level} takes a ".tsv" table file with samples raw count number, user needs to assign the control sample's name and case sample's name vector.
#' It will do a gene-level meta-analysis of screening data. User also need to assign value to \code{rna.size} to the most common number of sh/sgRNAs targeting one gene.
#' This function will remove bia caused by unbalanced number of sh/sgRNAs targeting the same gene.
#'
#' @param input.file, character, the file path of ".tsv" table with raw count number for each sample (column) and sh/sgRNA (row).
#' @param control.samples, vector of character, the control samples names, this information should match the column names of input.file. For example, c(T0_A, T0_B).
#' @param case.samples, vector of character, the case samples names, this information should match the column names of input.file. For example, c(T16_A, T16_B).
#' @param control.groupname, character, the name of your control samples' group name. For example, T0.
#' @param case.groupname, character, the name of your case samples' group name. For example, T16.
#' @param gene.columnId, integer, the number of the column which store the gene name. By default, 2.
#' @param data.type, character, can either be "microarray" or "NGS". Default is "microarray".
#' @param do.normalization, logical, if TRUE, a scaled normalization will be performed for each sample. To quantify this scale, user need to assign a value to \code{total}.
#' Default is TRUE.
#' @param total, integer, need to be larger than the colSums of the raw count table. Default is 1e6.
#' @param filterLowCount, logical, if TRUE, will remove rnas with low count, based on \code{filterBy} and \code{count.cutoff}. Default is TRUE.
#' @param filterBy, logical, if TRUE, will remove rnas with low count based on "control" samples or "case" samples. Default is "control".
#' @param count.cutoff, integer, the threshold of removing low count. Default is 4.
#' @param nitt, integer, the number of MCMC iterations. Default is 15000.
#' @param burnin, integer, burnin. Default is 5000.
#' @param thin, integer, thinning interval. Default is 10.
#' @param rna.size, integer, the most common number of sh/sgRNAs targetting the same gene. Default is 6.
#' @param sample.rna.time, integer, to remove unbalance number of sh/sgRNAs targetting the same gene. \code{rna.size} of sh/sgRNAs will be sampled, the sampling time.
#' @param method, character, estimation model. Either "Bayesian" or "MLE". Default is "Bayesian".
#' @param pooling, character, pooling method. Either "full" or "partial". Default is "partial".
#'
#' @return Return a data frame containing all the gene-level meta-analysis statistical values. Including geneID, log2FC, z value, p value, FDR and so on.
#'
#' @export
ScreenBEAM.gene.level<-function(input.file, control.samples, case.samples, control.groupname = "control", case.groupname = "treatment",
                                gene.columnId = 2, data.type = c("microarray", "NGS"), do.normalization = TRUE, total=1e6, filterLowCount = FALSE,
                                filterBy = "control", count.cutoff = 4, nitt = 15000, burnin = 5000, thin=10, rna.size=6, sample.rna.time=100, method = "Bayesian", pooling="partial", ...){
  eset <- generateEset.ScreenBEAM(input.file = input.file, control.samples = control.samples,
                                  case.samples = case.samples, control.groupname = control.groupname,
                                  case.groupname = case.groupname, gene.columnId = gene.columnId)
  DR <- DRAgeneLevel2(eset = eset, data.type = data.type, do.normalization = do.normalization,
                      filterLowCount = filterLowCount, filterBy = filterBy, total=total,
                      count.cutoff = count.cutoff, nitt = nitt, burnin = burnin, thin = thin, rna.size = rna.size, sample.rna.time = sample.rna.time, method=method, pooling=pooling,
                      ...)
  return(DR)
}
#' Create an HTML report to perform quality control of library, mapping status
#'
#' \code{ScreenBEAM.mapping.QC} takes the master \code{analysis.par} list, which contains all the key data through ScreenBEAM meta-analysis.
#' It will call Rmarkdown file to create an HTML report of library quality control (show the deleted duplicated/substring sh/sgRNA table). Percentage of mapping.
#' Suggested \code{rna.size}, which is the most common number of sh/sgRNAs targeting the same gene. A barplot at the end shows the suggested number of mismatch
#' for user to rescure the most reads.
#'
#' @param analysis.par, the master list, containing unique reads files path, blat result files path.
#' @param QC.Rmd.path, character, the path where user put the Rmarkdown file of "mapping_QC_report". Default, it will call the one in ScreenBEAM2 package.
#'
#' @return It will create an Rmarkdown file and an HTML QC report in the folder of \code{analsis.par$out.dir.output.QC}.
#'
#' @export
ScreenBEAM.mapping.QC<-function(analysis.par, QC.Rmd.path=system.file("Rmd","mapping_QC_report.Rmd", package = "ScreenBEAM2")){
  if(is.na(analysis.par$out.dir.output.QC)) stop("No output QC folder!")
  if(is.na(analysis.par$lib.name)) stop("No library information!")
  output_rmd_file <- sprintf("%s/mapping_%s_QC.Rmd", analysis.par$out.dir.output.QC, analysis.par$lib.name)
  file.copy(from = QC.Rmd.path,
            to = output_rmd_file, overwrite = T)
  rmarkdown::render(output_rmd_file, html_document(toc = TRUE))
}


#' Create a pdf plot to visualize the trim location.
#'
#' \code{ScreenBEAM.trim.helper} is a helper function for users who wants to know the most common trim place in their FASTQ reads.
#' This function requires the linux command \code{seqtk}. Please make sure you have it before calling this function.
#'
#' @param fastq.path, character, the file path of one FASTQ reads file. Unzipped.
#' @param sample_num, integer, the random sample number of FASTQ reads when creating the barplot. Default is 10000.
#' @param pdf.file, character, the file path to store the output pdf plot.
#'
#' @return It will create a PDF barplot file showing A,T,C,G and N's percentage at each base position. User can pick the trim location based on the plot.
#'
#' @export
ScreenBEAM.trim.helper<-function(fastq.path, sample_num=10000, pdf.file){
  # This dependent on seqtk command
  require(RColorBrewer)
  check.line.command<-paste0("wc -l <", fastq.path)
  total.line<-as.numeric((system(check.line.command, intern = TRUE)))
  fq.index<-seq(2,total.line-2,4)
  set.seed(123458)
  fq.index.sample<-sample(fq.index, 2)
  sample_num <- sample_num
  seqtk.command<-paste0("seqtk sample -s100 ", fastq.path, " ", sample_num, " > random_seq.txt")
  system(seqtk.command)
  x.in <- readLines("random_seq.txt")
  x.in <- x.in[seq(2, length(x.in), 4)] # every reads
  x.in <- x.in[!(x.in == "")]
  #Need to add # of random sample reads you need to get
  #fq.list<-scan("only_seq.txt", what = "", sep = "\n")
  fq.list.char<-strsplit(x.in, split = "")

  Mode <- function(x) {
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
  }

  fq.len<-Mode(sapply(fq.list.char, length))

  fq.loc.count.df<-data.frame(matrix(0, ncol=5, nrow = fq.len))
  colnames(fq.loc.count.df) <- c("A","C","G","T","N")
  rownames(fq.loc.count.df)<-c(seq(1, fq.len))

  for(index in 1:fq.len){
    for(read in fq.list.char){
      if(read[index]=="A") fq.loc.count.df[index,"A"] <- fq.loc.count.df[index,"A"]+1
      else if(read[index]=="C") fq.loc.count.df[index,"C"] <- fq.loc.count.df[index,"C"]+1
      else if(read[index]=="G") fq.loc.count.df[index,"G"] <- fq.loc.count.df[index,"G"]+1
      else if(read[index]=="T") fq.loc.count.df[index,"T"] <- fq.loc.count.df[index,"T"]+1
      else fq.loc.count.df[index,"N"] <- fq.loc.count.df[index,"N"]+1
    }
  }

  fq.loc.perc.df<-apply(fq.loc.count.df, 1, function(x){x*100/sum(x,na.rm = T)})
  bar.max<-unlist(apply(fq.loc.perc.df, 2, max))

  coul = brewer.pal(5, "Pastel2")
  pdf(pdf.file, width = 10, height =5)
  par(mar=rep(4,4))
  bar.plot<-barplot(fq.loc.perc.df, col=coul, border="white", xlab="Reads base index (5' to 3')",
                    ylab = "Nucleotide Percentage", main="Trimming Helper Plot", axisnames = F, width = 2)
  lines(x = bar.plot, y = bar.max)
  coord<-xy.coords(fq.loc.perc.df)
  text(x=bar.plot, y=-2, colnames(fq.loc.perc.df), cex=0.5, srt=90, xpd=TRUE)
  pp<-par()$usr
  legend(x = pp[2], y=pp[4],
         c("A", "C", "G", "T", "N"), xpd= TRUE, inset = c(0,0), pch = rep(15, length(coul)),col = coul, bty = "n")
  dev.off()
}





